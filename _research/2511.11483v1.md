---
abstract: Recent text-to-image (T2I) models have made remarkable progress in generating
  visually realistic and semantically coherent images. However, they still suffer
  from randomness and inconsistency with the given prompts, particularly when textual
  descriptions are vague or underspecified. Existing approaches, such as prompt rewriting,
  best-of-N sampling, and self-refinement, can mitigate these issues but usually require
  additional modules and operate independently, hindering test-time scaling efficiency
  and increasing computational overhead. In this paper, we introduce ImAgent, a training-free
  unified multimodal agent that integrates reasoning, generation, and self-evaluation
  within a single framework for efficient test-time scaling. Guided by a policy controller,
  multiple generation actions dynamically interact and self-organize to enhance image
  fidelity and semantic alignment without relying on external models. Extensive experiments
  on image generation and editing tasks demonstrate that ImAgent consistently improves
  over the backbone and even surpasses other strong baselines where the backbone model
  fails, highlighting the potential of unified multimodal agents for adaptive and
  efficient image generation under test-time scaling.
authors:
- Kaishen Wang
- Ruibo Chen
- Tong Zheng
- Heng Huang
layout: research_post
pdf: https://arxiv.org/pdf/2511.11483v1
title: 'ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image
  Generation'
year: 2025
---
Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.
