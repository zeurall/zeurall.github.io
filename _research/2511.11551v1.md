---
abstract: The deployment of decision-making AI agents presents a critical challenge
  in maintaining alignment with human values or guidelines while operating in complex,
  dynamic environments. Agents trained solely to achieve their objectives may adopt
  harmful behavior, exposing a key trade-off between maximizing the reward function
  and maintaining the alignment. For the pre-trained agents, ensuring alignment is
  particularly challenging, as retraining can be a costly and slow process. This is
  further complicated by the diverse and potentially conflicting attributes representing
  the ethical values for alignment. To address these challenges, we propose a test-time
  alignment technique based on model-guided policy shaping. Our method allows precise
  control over individual behavioral attributes, generalizes across diverse reinforcement
  learning (RL) environments, and facilitates a principled trade-off between ethical
  alignment and reward maximization without requiring agent retraining. We evaluate
  our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game
  environments and thousands of annotated scenarios involving ethical decisions. The
  RL agents are first trained to maximize the reward in their respective games. At
  test time, we apply policy shaping via scenario-action attribute classifiers to
  ensure decision alignment with ethical attributes. We compare our approach against
  prior training-time methods and general-purpose agents, as well as study several
  types of ethical violations and power-seeking behavior. Our results demonstrate
  that test-time policy shaping provides an effective and scalable solution for mitigating
  unethical behavior across diverse environments and alignment attributes.
authors:
- Dena Mujtaba
- Brian Hu
- Anthony Hoogs
- Arslan Basharat
layout: research_post
pdf: https://arxiv.org/pdf/2511.11551v1
title: 'Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping'
year: 2025
---
The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.
